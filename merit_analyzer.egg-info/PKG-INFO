Metadata-Version: 2.4
Name: merit-analyzer
Version: 1.0.0
Summary: AI system test failure analysis and recommendation engine
Home-page: https://github.com/merit-analyzer/merit-analyzer
Author: Merit Analyzer Team
Author-email: Merit Analyzer Team <team@merit-analyzer.com>
License: MIT
Keywords: ai,testing,analysis,claude,llm,debugging
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Testing
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: anthropic>=0.18.0
Requires-Dist: boto3>=1.26.0
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: click>=8.1.0
Requires-Dist: GitPython>=3.1.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Dynamic: author
Dynamic: home-page
Dynamic: requires-python

# Merit Analyzer

**AI system test failure analysis and recommendation engine**

Merit Analyzer is a Python SDK that analyzes AI system test results to provide specific, actionable recommendations for fixing failures. Built on the Claude Agent SDK, it discovers system architecture, identifies failure patterns, and generates targeted fixes across code, prompts, and agent architecture.

## ğŸš€ Key Features

- **Automatic Pattern Detection**: Clusters test failures into meaningful patterns
- **Architecture Discovery**: Uses Claude Code to understand your AI system structure
- **Root Cause Analysis**: Identifies the underlying causes of failures
- **Actionable Recommendations**: Provides specific, prioritized fixes
- **Multiple Output Formats**: JSON, Markdown, HTML reports
- **Framework Agnostic**: Works with any AI framework (LangChain, LlamaIndex, etc.)

## ğŸ“‹ What It Does

1. **Analyzes Test Results**: Takes your test failures and groups them into patterns
2. **Discovers Architecture**: Maps your AI system's components, prompts, and data flow
3. **Identifies Root Causes**: Determines why specific patterns are failing
4. **Generates Fixes**: Provides specific code changes, prompt improvements, and architectural recommendations
5. **Prioritizes Actions**: Ranks recommendations by impact and effort

## ğŸ¯ Target Users

- AI/ML engineers building agent systems
- Teams using LangChain, LlamaIndex, or custom LLM frameworks
- Companies running test suites on AI applications
- Developers debugging prompt/agent coordination issues

## ğŸ“¦ Installation

```bash
pip install merit-analyzer
```

## ğŸš€ Quick Start

### 1. Basic Usage

```python
from merit_analyzer import MeritAnalyzer, TestResult

# Your test results
test_results = [
    TestResult(
        test_id="test_001",
        input="How much does the pro plan cost?",
        expected_output="$49/month",
        actual_output="We have various pricing tiers",
        status="failed",
        failure_reason="Response too vague"
    ),
    # ... more tests
]

# Initialize analyzer
analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-...",  # Your Anthropic API key
    provider="anthropic"
)

# Run analysis
report = analyzer.analyze(test_results)

# View results
report.display()

# Save report
analyzer.save_report(report, "analysis_report.json")
```

### 2. Command Line Usage

```bash
# Basic analysis
merit-analyze --test-results test_results.json --api-key sk-ant-...

# With custom project path
merit-analyze --project-path ./my-ai-app --test-results results.json --output analysis.json

# Export recommendations separately
merit-analyze --test-results results.json --export-recommendations recs.md
```

### 3. Load Test Results from File

```python
from merit_analyzer import MeritAnalyzer

# Load from JSON file
analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-..."
)

# The analyzer can parse various formats
report = analyzer.analyze("test_results.json")
```

## ğŸ“Š Understanding the Output

### Analysis Report

The analysis report contains:

- **Summary Statistics**: Test counts, pass rates, patterns found
- **Failure Patterns**: Grouped failures with root cause analysis
- **Recommendations**: Specific, actionable fixes prioritized by impact
- **Action Plan**: Step-by-step implementation guide

### Example Output

```
ğŸ“Š ANALYSIS SUMMARY
====================================
Total tests: 15
Passed: 8
Failed: 7
Pass rate: 53.3%
Patterns found: 3
Recommendations: 12

ğŸ” FAILURE PATTERNS
------------------------------------
pricing_vague_responses: 4 failures (26.7%)
  Root cause: Prompt template lacks specific pricing examples

timeout_issues: 2 failures (13.3%)
  Root cause: No timeout handling for complex requests

validation_errors: 1 failures (6.7%)
  Root cause: Missing input validation

ğŸ’¡ TOP RECOMMENDATIONS
------------------------------------
1. Add specific pricing examples to prompt template
   Type: Prompt
   Effort: 30 minutes
   Impact: Fixes 4 tests in pricing_vague_responses

2. Implement timeout handling for complex requests
   Type: Code
   Effort: 1 hour
   Impact: Fixes 2 tests in timeout_issues
```

## ğŸ”§ Configuration

### Environment Variables

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
export MERIT_PROJECT_PATH="./my-ai-app"
export MERIT_PROVIDER="anthropic"
```

### Configuration File

Create `merit_config.yaml`:

```yaml
project_path: "./my-ai-app"
api_key: "sk-ant-..."
provider: "anthropic"
model: "claude-3-5-sonnet-20241022"
max_tokens: 4096
min_cluster_size: 2
max_patterns: 10
verbose: true
```

Use with:

```bash
merit-analyze --config merit_config.yaml --test-results results.json
```

## ğŸ“ Project Structure

```
merit-analyzer/
â”œâ”€â”€ merit_analyzer/
â”‚   â”œâ”€â”€ core/           # Core analysis engine
â”‚   â”œâ”€â”€ discovery/      # Project scanning and framework detection
â”‚   â”œâ”€â”€ analysis/       # Root cause analysis and Claude integration
â”‚   â”œâ”€â”€ recommendations/ # Recommendation generation and prioritization
â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â””â”€â”€ cli.py         # Command-line interface
â”œâ”€â”€ examples/           # Usage examples
â”œâ”€â”€ tests/             # Test suite
â””â”€â”€ docs/              # Documentation
```

## ğŸ§ª Supported Test Formats

Merit Analyzer supports multiple test result formats:

- **JSON**: Standard test result format
- **CSV**: Comma-separated values
- **pytest JSON**: pytest --json-report output
- **JUnit XML**: JUnit test results

### JSON Format

```json
[
  {
    "test_id": "test_001",
    "test_name": "pricing_inquiry",
    "input": "How much does the pro plan cost?",
    "expected_output": "$49/month",
    "actual_output": "We have various pricing tiers",
    "status": "failed",
    "failure_reason": "Response too vague",
    "category": "pricing",
    "tags": ["pricing", "pro_plan"],
    "execution_time_ms": 1250,
    "timestamp": "2024-01-15T10:30:00Z"
  }
]
```

## ğŸ—ï¸ Architecture

Merit Analyzer follows a layered architecture that processes test failures through multiple stages of analysis, from pattern detection to actionable recommendations.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Merit Analyzer SDK                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Input Layer                                       â”‚  â”‚
â”‚  â”‚    - Test result parser                              â”‚  â”‚
â”‚  â”‚    - Schema validation                               â”‚  â”‚
â”‚  â”‚    - Pattern detector (clustering)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. Discovery Layer (Claude Code)                     â”‚  â”‚
â”‚  â”‚    - Project structure analysis                      â”‚  â”‚
â”‚  â”‚    - Framework detection                             â”‚  â”‚
â”‚  â”‚    - Entry point identification                      â”‚  â”‚
â”‚  â”‚    - Agent/prompt discovery                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. Mapping Layer (Hybrid)                            â”‚  â”‚
â”‚  â”‚    - Pattern â†’ Code mapping                          â”‚  â”‚
â”‚  â”‚    - Failure clustering                              â”‚  â”‚
â”‚  â”‚    - Comparative analysis (pass vs fail)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 4. Analysis Layer (Claude Code)                      â”‚  â”‚
â”‚  â”‚    - Root cause analysis per pattern                 â”‚  â”‚
â”‚  â”‚    - Code review of relevant sections                â”‚  â”‚
â”‚  â”‚    - Prompt quality analysis                         â”‚  â”‚
â”‚  â”‚    - Architecture evaluation                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 5. Recommendation Engine (Claude Code + SDK)         â”‚  â”‚
â”‚  â”‚    - Generate specific fixes                         â”‚  â”‚
â”‚  â”‚    - Prioritize by impact                            â”‚  â”‚
â”‚  â”‚    - Estimate effort                                 â”‚  â”‚
â”‚  â”‚    - Format output                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 6. Output Layer                                      â”‚  â”‚
â”‚  â”‚    - Structured report (JSON/Markdown)               â”‚  â”‚
â”‚  â”‚    - Action plan                                     â”‚  â”‚
â”‚  â”‚    - Code diffs/suggestions                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                                    â†‘
         â”‚                                    â”‚
    User's API Key                    User's Codebase
   (Anthropic/Bedrock)               (analyzed by Claude Code)
```

### Technical Stack

#### Core Dependencies
- **Python**: 3.9+
- **Claude Agent SDK**: Latest version from Anthropic
- **Required packages**:
  - `anthropic` or `boto3` (for API access)
  - `scikit-learn` (for pattern clustering)
  - `numpy` (for similarity calculations)
  - `pydantic` (for data validation)
  - `rich` (for CLI output formatting)
  - `GitPython` (optional, for git integration)

#### Project Structure
```
merit-analyzer/
â”œâ”€â”€ merit_analyzer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ analyzer.py           # Main Analyzer class
â”‚   â”‚   â”œâ”€â”€ test_parser.py        # Parse test results
â”‚   â”‚   â”œâ”€â”€ pattern_detector.py   # Cluster failures
â”‚   â”‚   â””â”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ discovery/
â”‚   â”‚   â”œâ”€â”€ project_scanner.py    # Quick project analysis
â”‚   â”‚   â”œâ”€â”€ framework_detector.py # Detect LangChain, etc.
â”‚   â”‚   â””â”€â”€ code_mapper.py        # Map patterns to code
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ claude_agent.py       # Claude Code integration
â”‚   â”‚   â”œâ”€â”€ root_cause.py         # Root cause analysis
â”‚   â”‚   â””â”€â”€ comparative.py        # Compare pass/fail tests
â”‚   â”œâ”€â”€ recommendations/
â”‚   â”‚   â”œâ”€â”€ generator.py          # Generate recommendations
â”‚   â”‚   â”œâ”€â”€ prioritizer.py        # Prioritize fixes
â”‚   â”‚   â””â”€â”€ formatter.py          # Format output
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ test_result.py        # Pydantic models
â”‚       â”œâ”€â”€ pattern.py
â”‚       â”œâ”€â”€ recommendation.py
â”‚       â””â”€â”€ report.py
â”œâ”€â”€ tests/
â”œâ”€â”€ examples/
â”œâ”€â”€ docs/
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ” How It Works

### 1. Input Processing
- Parses and validates test results
- Detects failure patterns using clustering algorithms
- Groups similar failures together

### 2. Architecture Discovery
- Scans your codebase for AI components
- Detects frameworks (LangChain, LlamaIndex, etc.)
- Maps data flow and component relationships

### 3. Pattern Analysis
- Uses Claude Code to analyze each failure pattern
- Compares failing vs passing tests
- Identifies root causes and code issues

### 4. Recommendation Generation
- Generates specific, actionable fixes
- Prioritizes by impact and effort
- Provides implementation details

## ğŸ¯ Use Cases

### 1. Debugging AI Agent Failures
```python
# Analyze why your pricing agent is giving vague responses
report = analyzer.analyze(pricing_test_results)
# Get specific prompt improvements and code fixes
```

### 2. Improving Test Coverage
```python
# Find patterns in test failures to improve coverage
patterns = report.patterns
for pattern_name, pattern in patterns.items():
    print(f"Pattern: {pattern_name} - {pattern.failure_count} failures")
```

### 3. Performance Optimization
```python
# Identify timeout and performance issues
timeout_patterns = [p for p in patterns.values() if "timeout" in p.name]
```

### 4. Prompt Engineering
```python
# Get recommendations for improving prompts
prompt_recs = [r for r in report.recommendations if r.type.value == "prompt"]
```

## ğŸ› ï¸ Advanced Usage

### Custom Configuration

```python
from merit_analyzer import MeritAnalyzer

config = {
    "min_cluster_size": 3,
    "max_patterns": 15,
    "similarity_threshold": 0.4,
    "verbose": True
}

analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-...",
    config=config
)
```

### Integration with Test Frameworks

```python
# pytest integration example
import pytest
from merit_analyzer import MeritAnalyzer, TestResult

class MeritAnalyzerPlugin:
    def __init__(self):
        self.test_results = []
        self.analyzer = MeritAnalyzer(
            project_path=".",
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
    
    @pytest.hookimpl(hookwrapper=True)
    def pytest_runtest_makereport(self, item, call):
        outcome = yield
        report = outcome.get_result()
        
        if call.when == "call":
            test_result = TestResult(
                test_id=item.nodeid,
                test_name=item.name,
                input=item.funcargs.get('input', ''),
                actual_output=item.funcargs.get('output', ''),
                status="passed" if report.passed else "failed",
                failure_reason=str(report.longrepr) if report.failed else None
            )
            self.test_results.append(test_result)
    
    def pytest_sessionfinish(self, session):
        if any(t.status == "failed" for t in self.test_results):
            analysis_report = self.analyzer.analyze(self.test_results)
            analysis_report.display()
```

### Batch Processing

```python
# Analyze multiple test result files
test_files = ["results_1.json", "results_2.json", "results_3.json"]
all_reports = []

for test_file in test_files:
    report = analyzer.analyze(test_file)
    all_reports.append(report)
    print(f"Analyzed {test_file}: {len(report.recommendations)} recommendations")
```

## ğŸ“ˆ Performance

- **Analysis Time**: 2-10 minutes depending on codebase size
- **Token Usage**: 50K-500K tokens per analysis
- **Memory Usage**: ~100MB for typical projects
- **Supported Projects**: Up to 1000 Python files

## ğŸ”’ Security

- **No Data Retention**: All analysis is local or in your Claude account
- **API Key Security**: Never sent to Merit servers
- **File Exclusions**: Sensitive files can be excluded from analysis
- **On-Premise Support**: Can be deployed locally

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
git clone https://github.com/merit-analyzer/merit-analyzer.git
cd merit-analyzer
pip install -e ".[dev]"
pytest
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: [docs.merit-analyzer.com](https://docs.merit-analyzer.com)
- **Issues**: [GitHub Issues](https://github.com/merit-analyzer/merit-analyzer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/merit-analyzer/merit-analyzer/discussions)
- **Email**: support@merit-analyzer.com

## ğŸ—ºï¸ Roadmap

### Phase 1: MVP âœ…
- [x] Core analysis engine
- [x] Pattern detection
- [x] Claude Code integration
- [x] Basic recommendations

### Phase 2: Enhancement (Q2 2024)
- [ ] Web UI for report viewing
- [ ] More framework support
- [ ] Regression detection
- [ ] Git integration

### Phase 3: Advanced Features (Q3 2024)
- [ ] Automated fix generation
- [ ] Continuous monitoring
- [ ] CI/CD integration
- [ ] Team collaboration features

## ğŸ™ Acknowledgments

- Built on [Claude Agent SDK](https://github.com/anthropics/claude-agent-sdk)
- Inspired by the need for better AI system debugging tools
- Thanks to the open-source community for foundational libraries

---

**Transform test failures into specific code changes in minutes, not hours of manual debugging.**
