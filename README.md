# Merit Analyzer

**AI system test failure analysis and recommendation engine**

Merit Analyzer is a Python SDK that analyzes AI system test results to provide specific, actionable recommendations for fixing failures. Built on the Claude Agent SDK, it discovers system architecture, identifies failure patterns, and generates targeted fixes across code, prompts, and agent architecture.

## üöÄ Key Features

- **Automatic Pattern Detection**: Clusters test failures into meaningful patterns
- **Architecture Discovery**: Uses Claude Code to understand your AI system structure
- **Root Cause Analysis**: Identifies the underlying causes of failures
- **Actionable Recommendations**: Provides specific, prioritized fixes
- **Multiple Output Formats**: JSON, Markdown, HTML reports
- **Framework Agnostic**: Works with any AI framework (LangChain, LlamaIndex, etc.)

## üìã What It Does

1. **Analyzes Test Results**: Takes your test failures and groups them into patterns
2. **Discovers Architecture**: Maps your AI system's components, prompts, and data flow
3. **Identifies Root Causes**: Determines why specific patterns are failing
4. **Generates Fixes**: Provides specific code changes, prompt improvements, and architectural recommendations
5. **Prioritizes Actions**: Ranks recommendations by impact and effort

## üéØ Target Users

- AI/ML engineers building agent systems
- Teams using LangChain, LlamaIndex, or custom LLM frameworks
- Companies running test suites on AI applications
- Developers debugging prompt/agent coordination issues

## üì¶ Installation

```bash
pip install merit-analyzer
```

## üöÄ Quick Start

### 1. Basic Usage

```python
from merit_analyzer import MeritAnalyzer, TestResult

# Your test results
test_results = [
    TestResult(
        test_id="test_001",
        input="How much does the pro plan cost?",
        expected_output="$49/month",
        actual_output="We have various pricing tiers",
        status="failed",
        failure_reason="Response too vague"
    ),
    # ... more tests
]

# Initialize analyzer
analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-...",  # Your Anthropic API key
    provider="anthropic"
)

# Run analysis
report = analyzer.analyze(test_results)

# View results
report.display()

# Save report
analyzer.save_report(report, "analysis_report.json")
```

### 2. Command Line Usage

```bash
# Basic analysis
merit-analyze --test-results test_results.json --api-key sk-ant-...

# With custom project path
merit-analyze --project-path ./my-ai-app --test-results results.json --output analysis.json

# Export recommendations separately
merit-analyze --test-results results.json --export-recommendations recs.md
```

### 3. Load Test Results from File

```python
from merit_analyzer import MeritAnalyzer

# Load from JSON file
analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-..."
)

# The analyzer can parse various formats
report = analyzer.analyze("test_results.json")
```

## üìä Understanding the Output

### Analysis Report

The analysis report contains:

- **Summary Statistics**: Test counts, pass rates, patterns found
- **Failure Patterns**: Grouped failures with root cause analysis
- **Recommendations**: Specific, actionable fixes prioritized by impact
- **Action Plan**: Step-by-step implementation guide

### Example Output

```
üìä ANALYSIS SUMMARY
====================================
Total tests: 15
Passed: 8
Failed: 7
Pass rate: 53.3%
Patterns found: 3
Recommendations: 12

üîç FAILURE PATTERNS
------------------------------------
pricing_vague_responses: 4 failures (26.7%)
  Root cause: Prompt template lacks specific pricing examples

timeout_issues: 2 failures (13.3%)
  Root cause: No timeout handling for complex requests

validation_errors: 1 failures (6.7%)
  Root cause: Missing input validation

üí° TOP RECOMMENDATIONS
------------------------------------
1. Add specific pricing examples to prompt template
   Type: Prompt
   Effort: 30 minutes
   Impact: Fixes 4 tests in pricing_vague_responses

2. Implement timeout handling for complex requests
   Type: Code
   Effort: 1 hour
   Impact: Fixes 2 tests in timeout_issues
```

## üîß Configuration

### Environment Variables

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
export MERIT_PROJECT_PATH="./my-ai-app"
export MERIT_PROVIDER="anthropic"
```

### Configuration File

Create `merit_config.yaml`:

```yaml
project_path: "./my-ai-app"
api_key: "sk-ant-..."
provider: "anthropic"
model: "claude-3-5-sonnet-20241022"
max_tokens: 4096
min_cluster_size: 2
max_patterns: 10
verbose: true
```

Use with:

```bash
merit-analyze --config merit_config.yaml --test-results results.json
```

## üìÅ Project Structure

```
merit-analyzer/
‚îú‚îÄ‚îÄ merit_analyzer/
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Core analysis engine
‚îÇ   ‚îú‚îÄ‚îÄ discovery/      # Project scanning and framework detection
‚îÇ   ‚îú‚îÄ‚îÄ analysis/       # Root cause analysis and Claude integration
‚îÇ   ‚îú‚îÄ‚îÄ recommendations/ # Recommendation generation and prioritization
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Data models
‚îÇ   ‚îî‚îÄ‚îÄ cli.py         # Command-line interface
‚îú‚îÄ‚îÄ examples/           # Usage examples
‚îú‚îÄ‚îÄ tests/             # Test suite
‚îî‚îÄ‚îÄ docs/              # Documentation
```

## üß™ Supported Test Formats

Merit Analyzer supports multiple test result formats:

- **JSON**: Standard test result format
- **CSV**: Comma-separated values
- **pytest JSON**: pytest --json-report output
- **JUnit XML**: JUnit test results

### JSON Format

```json
[
  {
    "test_id": "test_001",
    "test_name": "pricing_inquiry",
    "input": "How much does the pro plan cost?",
    "expected_output": "$49/month",
    "actual_output": "We have various pricing tiers",
    "status": "failed",
    "failure_reason": "Response too vague",
    "category": "pricing",
    "tags": ["pricing", "pro_plan"],
    "execution_time_ms": 1250,
    "timestamp": "2024-01-15T10:30:00Z"
  }
]
```

## üèóÔ∏è Architecture

Merit Analyzer follows a layered architecture that processes test failures through multiple stages of analysis, from pattern detection to actionable recommendations.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Merit Analyzer SDK                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 1. Input Layer                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Test result parser                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Schema validation                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Pattern detector (clustering)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 2. Discovery Layer (Claude Code)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Project structure analysis                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Framework detection                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Entry point identification                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Agent/prompt discovery                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 3. Mapping Layer (Hybrid)                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Pattern ‚Üí Code mapping                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Failure clustering                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Comparative analysis (pass vs fail)             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 4. Analysis Layer (Claude Code)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Root cause analysis per pattern                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Code review of relevant sections                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Prompt quality analysis                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Architecture evaluation                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 5. Recommendation Engine (Claude Code + SDK)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Generate specific fixes                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Prioritize by impact                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Estimate effort                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Format output                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 6. Output Layer                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Structured report (JSON/Markdown)               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Action plan                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - Code diffs/suggestions                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üë                                    ‚Üë
         ‚îÇ                                    ‚îÇ
    User's API Key                    User's Codebase
   (Anthropic/Bedrock)               (analyzed by Claude Code)
```

### Technical Stack

#### Core Dependencies
- **Python**: 3.9+
- **Claude Agent SDK**: Latest version from Anthropic
- **Required packages**:
  - `anthropic` or `boto3` (for API access)
  - `scikit-learn` (for pattern clustering)
  - `numpy` (for similarity calculations)
  - `pydantic` (for data validation)
  - `rich` (for CLI output formatting)
  - `GitPython` (optional, for git integration)

#### Project Structure
```
merit-analyzer/
‚îú‚îÄ‚îÄ merit_analyzer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py           # Main Analyzer class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_parser.py        # Parse test results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pattern_detector.py   # Cluster failures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ discovery/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project_scanner.py    # Quick project analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ framework_detector.py # Detect LangChain, etc.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ code_mapper.py        # Map patterns to code
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude_agent.py       # Claude Code integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ root_cause.py         # Root cause analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comparative.py        # Compare pass/fail tests
‚îÇ   ‚îú‚îÄ‚îÄ recommendations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py          # Generate recommendations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prioritizer.py        # Prioritize fixes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ formatter.py          # Format output
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ test_result.py        # Pydantic models
‚îÇ       ‚îú‚îÄ‚îÄ pattern.py
‚îÇ       ‚îú‚îÄ‚îÄ recommendation.py
‚îÇ       ‚îî‚îÄ‚îÄ report.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ examples/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ README.md
```

## üìö API Reference

### TestResult Schema

The `TestResult` model defines the structure for test data input to Merit Analyzer.

#### Required Fields
- `test_id` (str): Unique identifier for the test
- `input` (str): Input to the AI system
- `actual_output` (str): Actual response from system
- `status` (str): Test result status - one of: "passed", "failed", "error", "skipped"

#### Optional Fields
- `test_name` (str, optional): Human-readable test name
- `expected_output` (str, optional): Expected response
- `failure_reason` (str, optional): Why the test failed ‚ö†Ô∏è **Highly recommended for better pattern detection**
- `category` (str, optional): Test category (e.g., 'pricing', 'support')
- `tags` (list[str], optional): Tags for grouping (defaults to empty list)
- `execution_time_ms` (int, optional): Test execution time in milliseconds
- `timestamp` (str, optional): ISO timestamp
- `trace` (dict, optional): Execution trace/logs
- `metadata` (dict, optional): Additional metadata

> **üí° Tip**: While `failure_reason` is optional, providing it significantly improves pattern detection quality. The system will auto-generate basic failure context when missing, but explicit failure reasons lead to more accurate clustering.

#### Example Usage

```python
# Minimal test result (only required fields)
test_result = TestResult(
    test_id="test_001",
    input="What is the capital of France?",
    actual_output="The capital of France is Paris.",
    status="passed"
)

# Complete test result with all fields
test_result = TestResult(
    test_id="test_002",
    test_name="Greeting Test",
    input="Generate a greeting",
    expected_output="Hello! How can I help you today?",
    actual_output="Hello! How can I help you?",
    status="failed",
    failure_reason="Missing sign-off",
    category="greeting",
    tags=["user_interaction", "formatting"],
    execution_time_ms=1500,
    timestamp="2024-01-15T10:30:00Z",
    metadata={"environment": "production", "version": "1.2.3"}
)
```

## üîç How It Works

### 1. Input Processing
- Parses and validates test results
- Detects failure patterns using clustering algorithms
- Groups similar failures together

### 2. Architecture Discovery
- Scans your codebase for AI components
- Detects frameworks (LangChain, LlamaIndex, etc.)
- Maps data flow and component relationships

### 3. Pattern Analysis
- Uses Claude Code to analyze each failure pattern
- Compares failing vs passing tests
- Identifies root causes and code issues

### 4. Recommendation Generation
- Generates specific, actionable fixes
- Prioritizes by impact and effort
- Provides implementation details

## üéØ Use Cases

### 1. Debugging AI Agent Failures
```python
# Analyze why your pricing agent is giving vague responses
report = analyzer.analyze(pricing_test_results)
# Get specific prompt improvements and code fixes
```

### 2. Improving Test Coverage
```python
# Find patterns in test failures to improve coverage
patterns = report.patterns
for pattern_name, pattern in patterns.items():
    print(f"Pattern: {pattern_name} - {pattern.failure_count} failures")
```

### 3. Performance Optimization
```python
# Identify timeout and performance issues
timeout_patterns = [p for p in patterns.values() if "timeout" in p.name]
```

### 4. Prompt Engineering
```python
# Get recommendations for improving prompts
prompt_recs = [r for r in report.recommendations if r.type.value == "prompt"]
```

## üõ†Ô∏è Advanced Usage

### Custom Configuration

```python
from merit_analyzer import MeritAnalyzer

config = {
    "min_cluster_size": 3,
    "max_patterns": 15,
    "similarity_threshold": 0.4,
    "verbose": True
}

analyzer = MeritAnalyzer(
    project_path="./my-ai-app",
    api_key="sk-ant-...",
    config=config
)
```

### Integration with Test Frameworks

```python
# pytest integration example
import pytest
from merit_analyzer import MeritAnalyzer, TestResult

class MeritAnalyzerPlugin:
    def __init__(self):
        self.test_results = []
        self.analyzer = MeritAnalyzer(
            project_path=".",
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
    
    @pytest.hookimpl(hookwrapper=True)
    def pytest_runtest_makereport(self, item, call):
        outcome = yield
        report = outcome.get_result()
        
        if call.when == "call":
            test_result = TestResult(
                test_id=item.nodeid,
                test_name=item.name,
                input=item.funcargs.get('input', ''),
                actual_output=item.funcargs.get('output', ''),
                status="passed" if report.passed else "failed",
                failure_reason=str(report.longrepr) if report.failed else None
            )
            self.test_results.append(test_result)
    
    def pytest_sessionfinish(self, session):
        if any(t.status == "failed" for t in self.test_results):
            analysis_report = self.analyzer.analyze(self.test_results)
            analysis_report.display()
```

### Batch Processing

```python
# Analyze multiple test result files
test_files = ["results_1.json", "results_2.json", "results_3.json"]
all_reports = []

for test_file in test_files:
    report = analyzer.analyze(test_file)
    all_reports.append(report)
    print(f"Analyzed {test_file}: {len(report.recommendations)} recommendations")
```

## üìà Performance

- **Analysis Time**: 2-10 minutes depending on codebase size
- **Token Usage**: 50K-500K tokens per analysis
- **Memory Usage**: ~100MB for typical projects
- **Supported Projects**: Up to 1000 Python files

## üîí Security

- **No Data Retention**: All analysis is local or in your Claude account
- **API Key Security**: Never sent to Merit servers
- **File Exclusions**: Sensitive files can be excluded from analysis
- **On-Premise Support**: Can be deployed locally

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
git clone https://github.com/merit-analyzer/merit-analyzer.git
cd merit-analyzer
pip install -e ".[dev]"
pytest
```

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üÜò Support

- **Documentation**: [docs.merit-analyzer.com](https://docs.merit-analyzer.com)
- **Issues**: [GitHub Issues](https://github.com/merit-analyzer/merit-analyzer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/merit-analyzer/merit-analyzer/discussions)
- **Email**: support@merit-analyzer.com

## üó∫Ô∏è Roadmap

### Phase 1: MVP ‚úÖ
- [x] Core analysis engine
- [x] Pattern detection
- [x] Claude Code integration
- [x] Basic recommendations

### Phase 2: Enhancement (Q2 2024)
- [ ] Web UI for report viewing
- [ ] More framework support
- [ ] Regression detection
- [ ] Git integration

### Phase 3: Advanced Features (Q3 2024)
- [ ] Automated fix generation
- [ ] Continuous monitoring
- [ ] CI/CD integration
- [ ] Team collaboration features

## üôè Acknowledgments

- Built on [Claude Agent SDK](https://github.com/anthropics/claude-agent-sdk)
- Inspired by the need for better AI system debugging tools
- Thanks to the open-source community for foundational libraries

---

**Transform test failures into specific code changes in minutes, not hours of manual debugging.**
